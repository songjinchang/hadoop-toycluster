version5 this document is updated on 2020-1-12 by wangfengdev@163.com
version6 updated 2020-1-30 by wangfengdev@163.com

******************************
*                            *
*       Install Hadoop       *
*                            *
******************************

Ubuntu 64bit 16.04

00.every PCs use fix ipaddress
in desktop righttop corner 
->edit connection
->Wired connection 1
->Edit
->IPv4 Setting
->Method: Mannual
->Addresses: Add
->edit: address 192.168.10.xxx 
        netmask 255.255.255.0
	Gateway 192.168.10.1
        DNS Servers 192.168.10.1
->Save
->Disconnect
->reconnect by click Wired connection1
in terminal:
ifconfig check ipaddress is ok.



0.update apt-get
use mirrors.aliyun.com to replace host(cn.archive.ubuntu.com) in /etc/apt/sources.list
you can not edit this list file directly. you should copy list file in home dir, edit it and use 'sudo' to copy back to /etc/apt/sources.list.
sudo cp ~/sources.list /etc/apt/sources.list


1.install ssh
sudo apt-get update
sudo apt-get install ssh

{
	IF some lock error happens , use this:
	sudo rm /var/cache/apt/archives/lock
	sudo rm /var/lib/dpkg/lock
}

sudo apt-get install openssh-server
sudo apt-get install rsync
[in user homedir]$ mkdir .ssh
cd .ssh
ssh-keygen -t rsa
一直按enter键
cat ./id_rsa.pub >> ./authorized_keys  
ssh localhost 
when ask[yes/no] use yes.
就不再需要密码了

==========2.oracle jdk ========
1.tar -zxf jdk-8u192-linux-x64.tar.gz
2.sudo mv jdk1.8.0_192 /usr/local
3.nano ~/.bashrc
ADD
export JAVA_HOME=/usr/local/jdk1.8.0_192
export PATH=$PATH:$JAVA_HOME/bin

4.source ~/.bashrc
5.java -version     
========== oracle jdk ========


3.install hadoop2.7.6
tar -zxvf hadoop-2.7.6.tar.gz
sudo mv hadoop-2.7.6 /usr/local/hadoop
sudo chmod 777 /usr/local/hadoop

4.modify /usr/local/hadoop/etc/hadoop/hadoop-env.sh
nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh
replace: export JAVA_HOME=${JAVA_HOME}
to: export JAVA_HOME=/usr/local/jdk1.8.0_192

5.add hadoop to path
add following lines into ~/.bashrc
nano ~/.bashrc
export PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin
source ~/.bashrc
hadoop version

[single mode is ok. no namenode , no datanode]


[NOTE!!]
If you want to use multi-mode, please do everything above for every PCs.
Then follow the steps in <<<Multi-Node Setup>>>


[below is for pseu distribution mode one namenode and datanode in one machine.]

6.0.1 make dir
mkdir ~/tempdir
mkdir ~/namedir
mkdir ~/datadir

6.edit core-site.xml

<configuration>
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/home/hadoop/tempdir</value>
	</property>
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://ubuntu:9000</value>
	</property>
</configuration>

7.edit hdfs-site.xml

<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.block.size</name>
        <value>134217728</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/home/hadoop/namedir</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/home/hadoop/datadir</value>
    </property>
</configuration>

7.1 Edit hadoop-env.sh make more heap size
export HADOOP_CLIENT_OPTS="-Xmx1024m $HADOOP_CLIENT_OPTS"
It seems hadoop need at least 1024m to process a 128MB data array.

7.2 slaves
ubuntu

7.3 edit vi /etc/hosts in this machine(ubuntu).
sudo nano /etc/hosts
192.168.146.129 ubuntu


8.format (first time)
hadoop namenode -format


9.start
start-dfs.sh

10.web[ http://localhost:50070 ]

11.stop
stop-dfs.sh

if something is wrong, please check hadoop/etc/hadoop/logs


************************************************
**                                            **
**           Multi-Node Setup                 **
**                                            **
************************************************
<<<Multi-Node Setup>>>
II Multi-Node Setup
0.write down every computes' hostname and ipv4 address.
open a terminal , you will see hostname after hadoop@xxx:
xxx is your hostname.
use ifconfig to see the ipv4 address.


1.get all nodes ip addresses.

2.edit vi /etc/hosts in all nodes.
192.168.10.10 hp
192.168.10.11 hadoop-master
192.168.10.12 hadoop-HP-Laptop-14s
192.168.10.13 hadoop-X200-2
192.168.10.14 hadoop-X200
192.168.10.20 hadoop-i3-2
192.168.10.21 hadoop-Lenovo1201
192.168.10.22 hadoop-Lenovo1202

copy hosts to every PCs:
scp /etc/hosts hadoop@hp:hosts.txt
scp /etc/hosts hadoop@hadoop-X200-2:hosts.txt
scp /etc/hosts hadoop@hadoop-X200:hosts.txt

scp /etc/hosts hadoop@hadoop-HP-Laptop-14s:hosts.txt

scp /etc/hosts hadoop@hadoop-i3-2:hosts.txt
scp /etc/hosts hadoop@hadoop-Lenovo1201:hosts.txt
scp /etc/hosts hadoop@hadoop-Lenovo1202:hosts.txt

in every PCs copy hosts.txt to /etc/hosts
sudo cp hosts.txt /etc/hosts

3.No key login for each nodes
If you use ssh-keygen -t rsa before, then you should ignore step 3.1
For master node:
(3.1) ssh-keygen -t rsa [use enter for none input,if you already has one key then skip this step]

(3.2) copy master pub key to every PCs.
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hp
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-master
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-HP-Laptop-14s
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-X200-2
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-X200
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-i3-2
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-Lenovo1201
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-Lenovo1202

(3.3) chmod 0600 ~/.ssh/authorized_keys
use ssh hadoop@hadoop-slave-1 to check if it is succ.
Login on each slave nodes, and do the same as master.
For slave nodes:
(3.1) ssh-keygen -t rsa [use enter for none input,if you already has one key then skip this step]
(3.2) copy node pub key to every others.
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hp
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-master
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-HP-Laptop-14s
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-X200-2
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-X200
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-i3-2
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-Lenovo1201
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-Lenovo1202
(3.3) chmod 0600 ~/.ssh/authorized_keys
 
**************************************
**                                  **
**  Architecture of a Hadoop Cluster**
**                                  **
**************************************
A job can be split into tasks(map or reduce).

Hadoop:
{
	NameNode(master) 
	dataNode(slaves or workders).
}

Yarn:
yarn-actor:
{
	ResourceManager(in toycluster in master. manage the whole cluster.)
	NodeManager(in every worker node. manage AM and tasks run in the node.)
}

AM, container, etc.
1.a job is start by client
2.yarn-resourceManager ask one nodeManager to create a application-master(AM) in charge.
3.AM create executors for tasks.
4.both AM and executors are run in containers, which is controlled by NodeManager.





**************************************
**                                  **
**  About memories settings:        **
**                                  **
**************************************
About memories settings:
ref:https://www.linode.com/docs/databases/hadoop/how-to-install-and-set-up-hadoop-cluster/#architecture-of-a-hadoop-cluster

[1]
yarn-site.xml->yarn.nodemanager.resource.memory-mb decide all mem can be allocated in one single machine for all-yarn-containers in this machine.

[2]
yarn-site.xml->yarn.scheduler.maximum-allocation-mb 
yarn-site.xml->yarn.scheduler.minimum-allocation-mb
these two value decide how much one single container can consume.

[3]
mapred-site.xml with yarn.app.mapreduce.am.resource.mb
it is decide how much AM can consume mem. it should be between  yarn.scheduler.maximum-allocation-mb and yarn-site.xml->yarn.scheduler.minimum-allocation-mb.

[4]
mapred-site.xml->mapreduce.map.memory.mb 
mapred-site.xml->mapreduce.reduce.memory.mb
these two value decide how much one map/reduce operation can consume mem.
A map/reduce runs in one container, so map/reduce mem should be smaller than container.

Graph:
Machine(worker/node)
------------------------
|  NodeManager          |     
|                       |  
|  [container1 run AM]  |
|  [container2 run map] |
|  [container3 run Red] |
|                       |
------------------------

For A 2GB mem machine setting should be[by ref]:
yarn.nodemanager.resource.memory-mb	1536
yarn.scheduler.maximum-allocation-mb	1536
yarn.scheduler.minimum-allocation-mb	128
yarn.app.mapreduce.am.resource.mb	512
mapreduce.map.memory.mb     256
mapreduce.reduce.memory.mb	256

use ALT+CTRL+F1-F7 switch shell-mode/GUI-mode
in shell-mode it seems the whole system only use 400MB.

The ref value is not good. I give the settings(2GB):
yarn.nodemanager.resource.memory-mb	1536
yarn.scheduler.maximum-allocation-mb	1536
yarn.scheduler.minimum-allocation-mb	768
yarn.app.mapreduce.am.resource.mb	768
mapreduce.map.memory.mb     768
mapreduce.reduce.memory.mb  768

for 4GB machine, I give:
yarn.nodemanager.resource.memory-mb	3072
yarn.scheduler.maximum-allocation-mb	3072
yarn.scheduler.minimum-allocation-mb	768
yarn.app.mapreduce.am.resource.mb	768
mapreduce.map.memory.mb     768
mapreduce.reduce.memory.mb  768




**************************************
**                                  **
**  Install Hadoop on master        **
**                                  **
**************************************
<<<install hadoop on master>>>
4.install hadoop on master
make tempdir,namedir,datadir under /home/hadoop/
edit some config files:
/usr/local/hadoop/etc/hadoop

core-site.xml 
<configuration>
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/home/hadoop/tempdir</value>
	</property>
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://hadoop-master:9000</value>
	</property>
</configuration>


hdfs-site.xml
<configuration>
	<property>
		<name>dfs.replication</name>
		<value>3</value>
	</property>
	<property>
		<name>dfs.namenode.name.dir</name>
		<value>file:/home/hadoop/namedir</value>
	</property>
	<property>
		<name>dfs.datanode.data.dir</name>
		<value>file:/home/hadoop/datadir</value>
	</property>
</configuration>


mapred-site.xml (use yarn)
<configuration>
	<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	</property>
	<property>
		<name>yarn.app.mapreduce.am.resource.mb</name>
		<value>700</value>
		<description>MR_App_can_use_mem_count</description>
	</property>
	<property>
		<name>mapreduce.map.memory.mb</name>
		<value>700</value>
	</property>
	<property>
		<name>mapreduce.reduce.memory.mb</name>
		<value>700</value>
	</property>
	<property>
		<name>mapred.child.java.opts</name>
		<value>-Xmx700m</value>
		<description>only_java_can_use_mem_count</description>
	</property>
</configuration>

Edit yarn-site.xml
<configuration>
    <property>
		<name>yarn.acl.enable</name>
		<value>0</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop-master</value>
	    <description>who_run_resourcemanager</description>
    </property>
    <property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
 	    <description>NodeManager_can_run_MapReduce</description>
    </property>
	<property>
		<name>yarn.nodemanager.resource.memory-mb</name>
		<value>2800</value>
		<description>total_mem_a_node_can_use</description>
	</property>
	<property>
		<name>yarn.scheduler.maximum-allocation-mb</name>
		<value>700</value>
		<description>one_task_can_use_max_mem</description>
	</property>
	<property>
		<name>yarn.scheduler.minimum-allocation-mb</name>
		<value>700</value>
		<description>one_task_can_use_min_mem</description>
	</property>
	<property>
		<name>yarn.nodemanager.vmem-check-enabled</name>
		<value>false</value>
		<description>check_is_pc_use_virtualMem</description>
	</property>
	<property>
		<name>yarn.nodemanager.resource.cpu-vcores</name>
		<value>2</value>
		<description>avail_cpu_core_num</description>
	</property> 
	<property>
		<name>yarn.scheduler.minimum-allocation-vcores</name>
		<value>1</value>
		<description>one_container_min_cpu</description>
	</property>
	<property>
		<name>yarn.scheduler.maximum-allocation-vcores</name>
		<value>1</value>
		<description>one_container_max_cpu</description>
	</property>
	<property>
		<name>yarn.nodemanager.vmem-pmem-ratio</name>
		<value>2.1</value>
	</property>
</configuration>



Edit /usr/local/hadoop/etc/hadoop/salves
hp
hadoop-master
hadoop-HP-Laptop-14s
hadoop-X200-2
hadoop-X200
hadoop-i3-2
hadoop-Lenovo1201
hadoop-Lenovo1202

[Note!!]
in hadoop3.0 the slaves is called workers.


**************************************
**                                  **
**  Install Hadoop on Slaves        **
**                                  **
**************************************
<<<Install Hadoop on Slaves>>>
5.copy configured hadoop to slave
in master
tar czf ~/hadoop.tar.gz /usr/local/hadoop
scp ~/hadoop.tar.gz hadoop-slave-1:~
scp ~/hadoop.tar.gz hadoop-slave-2:~

6.un-tar configured hadoop on all slaves
tar xzf hadoop.tar.gz
mv hadoop /usr/local/hadoop


in fact, only copy settings-xml files to slaves are ok.
cd /usr/local/hadoop/etc
tar czf hadoop-etc.tar.gz hadoop

scp hadoop-etc.tar.gz hadoop@hp:~
scp hadoop-etc.tar.gz hadoop@hadoop-HP-Laptop-14s:~
scp hadoop-etc.tar.gz hadoop@hadoop-X200-2:~
scp hadoop-etc.tar.gz hadoop@hadoop-X200:~
scp hadoop-etc.tar.gz hadoop@hadoop-i3-2:~
scp hadoop-etc.tar.gz hadoop@hadoop-Lenovo1201:~
scp hadoop-etc.tar.gz hadoop@hadoop-Lenovo1202:~

tar xzf hadoop-etc.tar.gz 
cp -rf hadoop /usr/local/hadoop/etc
rm ~/hadoop-etc.tar.gz
rm -rf ~/hadoop

for each machine edit mem settings:
nano /usr/local/hadoop/etc/hadoop/yarn-site.xml
nano /usr/local/hadoop/etc/hadoop/mapred-site.xml
check free memories use: free

[NOTE!]
Without GUI ubuntu only cost 350MB memories.

Boot ubuntu16.04 without GUI
sudo systemctl disable lightdm.service

Reactive GUI
sudo systemctl enable lightdm.service
sudo systemctl start lightdm.service



7.in master Start hadoop cluster

hdfs namenode -format
start-dfs.sh
start-yarn.sh

8.check daemons
for master
jps
NameNode
ResourceManager

for slaves
DataNode
NodeManager


9.Stop hadoop ( run on master )
stop-yarn.sh
stop-dfs.sh

shutdown machine:
shutdown -h now

10. DFS WEB Page
http://localhost:50070/explorer.html#/
Yarn WEB Page
http://hadoop-master:8088/cluster
HDFS MB is 1024 Kbytes * 1024 , 128MB is 134217728Bytes


****
uninstall openjdk
sudo apt-get remove openjdk*


install Oracle JDK
1.tar -zxf jdk-8u191-linux-i586.tar.gz
2.sudo mv jdk1.8.0_191 /usr/local
3.nano ~/.bashrc
ADD
export JAVA_HOME=/usr/local/jdk1.8.0_191
export PATH=$PATH:$JAVA_HOME/bin

4.source ~/.bashrc
5.java -version

6.Edit 
export JAVA_HOME=/usr/local/jdk1.8.0_191
7.Edit .bashrc
export HADOOP_COMMON_LIB_NATIVE_DIR=/usr/local/hadoop/lib/native
export HADOOP_OPTS="-Djava.library.path=/usr/local/hadoop/lib/native"
export LD_LIBRARY_PATH=/usr/lib/hadoop/lib/native
 



***** DEV-1
build a MapReduce Program At Least Import following libs:
hadoop-common-2.7.6.jar
hadoop-hdfs-2.7.6.jar
hadoop-mapreduce-client-core-2.7.6.jar

***** DEV-2
Java Heap Error
use :

yarn-site.xml
<property>
<name>yarn.nodemanager.vmem-pmem-ratio</name>
<value>4.2</value>
</property>

mapred-site.xml
<property>
<name>mapred.child.java.opts</name>
<value>-Xmx2048m</value>
</property>

**** DEV-3
total 6400MB 
hadoop(3 node , each 1vcore) use 360 seconds
hadoop(3 node , each 2vcore) use 120 seconds
C++ theratically use 6400/60=106



***** MR4C-1
I feel MR4C is a dead path.

=============================
Try more vcores
yarn-site.xml 2800,900,900 vcores 3 2 3
hadoop-env.sh  3000
mapred-site.xml  map red java 900mb
try 1
18:13 map 0% red 0%
20:29 map 100% red 100%
try 2
23:10 map 0% red 0%
25:07 map 100% red 100%
try 3
26:48 map 0 red 0
28:59 map 100 red 100
===========================
2018-1-8 4:30

Try more vcores
yarn-site.xml 2800,700,700 vcores 4 2 4
hadoop-env.sh  3000
mapred-site.xml  map red java 700mb
try-1
42:00 0 0
44:00 100 100
try-2
45:45 0 0
47:35 100 100
try-3
48:40 0 0 
50:19 100 100
total 99 sec
try-4
52:32 0 0 
54:08 100 100
total 96 sec

clear caches
sudo sh -c "sync; echo 3 > /proc/sys/vm/drop_caches"

Try single c++ program read 6 gb.
73-75 seconds

=================
2018-1-8 05:30
Try :
1. data count in record reader step.
2. not copy 128mb data into bytes-writable for mapper.
3. only send results int to mapper.

try-1
57:34 0 0
59:11 100 100
total 97 seconds

try-2
00:22 0 0 
02:21 100 100

try-3
03:40 0 0 
05:14 100 100
total 94 seconds

it seems use byteswritable in mapper does not take times.

**************************************
**                                  **
**  Install SPARK                   **
**                                  **
**************************************
https://www.edureka.co/blog/spark-tutorial/
https://zh.hortonworks.com/tutorial/setting-up-a-spark-development-environment-with-scala/
https://www.tutorialspoint.com/apache_spark/apache_spark_installation.htm (install scala in terminal)

1.install java 1.8 SDK above , this should be done in hadoop.
2.install scala
download scala-2.11.6.tgz
2.1,2.2
tar xvf scala-2.11.6.tgz
sudo mv scala-2.11.6 /usr/local/scala



3.install spark
3.1,3.2 
tar xvf spark-2.1.0-bin-hadoop2.7.tgz 
sudo mv spark-2.1.0-bin-hadoop2.7 /usr/local/spark 

3.3
nano ~/.bashrc 
export PATH=$PATH:/usr/local/scala/bin 
export PATH=$PATH:/usr/local/spark/bin:/usr/local/spark/sbin

3.4 source ~/.bashrc

3.5 scala -version
3.6 spark-shell
ctrl+c to quit

4.config spark
chmod 777 /usr/local/spark
chmod 777 /usr/local/scala
mkdir /home/hadoop/spark-tmp
cd /usr/local/spark/conf
cp spark-env.sh.template spark-env.sh
cp spark-defaults.conf.template spark-defaults.conf
cp slaves.template slaves

4.3 edit
nano spark-env.sh
export SCALA_HOME=/usr/local/scala
export JAVA_HOME=/usr/local/jdk1.8.0_192
export SPARK_MASTER_HOST=hadoop-master
export SPARK_MASTER_PORT=7077
export SPARK_WORKER_DIR=/home/hadoop/spark-tmp
export SPARK_EXECUTOR_INSTANCES=4
export SPARK_EXECUTOR_CORES=1
export SPARK_EXECUTOR_MEMORY=512m
export SPARK_LOCALITY_WAIT=60s
export SPARK_LOCALITY_WAIT_PROCESS=0s
SPARK_LOCAL_HOSTNAME=<data node hostname>
SPARK_LOCAL_IP=<data node ip>

SPARK_LOCALITY_WAIT is the time(sec) executor wait to start local-data task. If it is exceed the time, executor send task to a less-node node to run.

SPARK_LOCALITY_WAIT_PROCESS, I am not very clear about this one. It give the process wait time(sec) for cache data.


4.4
nano spark-defaults.conf
spark.master  spark://hadoop-master:7077

4.5
nano slaves
hp
hadoop-master
hadoop-HP-Laptop-14s
hadoop-X200-2
hadoop-X200
hadoop-i3-2  20
hadoop-Lenovo1201  21
hadoop-Lenovo1202  22

5 done
start spark:
# auto start spark, not recommended.
/usr/local/spark/sbin/start-all.sh

# manual start master:
/usr/local/spark/sbin/start-master.sh
# manual start worker:
/usr/local/spark/sbin/start-slave.sh -c 1 -h hostname spark://MasterName:7077

check spark website: http://localhost:8080/

stop spark: 
/usr/local/spark/sbin/stop-all.sh
stop-yarn.sh
stop-dfs.sh

===spark examples:
spark-shell
val textfile=sc.textFile("hdfs://hadoop-master:9000/test.txt")
textfile.count()
val words=textfile.flatMap(line=>line.split(" "))
words.coutn()
words.first()

val bin1=sc.binaryRecords("hdfs://hadoop-master:9000/text.txt",1)
bin1.first()
val bin2=bin1.map(x=>x)

val allfiles=sc.binaryRecords("hdfs://hadoop-master:9000/demo128",2)
allfiles.count()
use almost 180-240 seconds

try one block file speed.
val onefile=sc.binaryRecords("hdfs://hadoop-master:9000/demo128/image_0_0_4096_4096_4_i16_envi",2)
onefile.count()
use 21 seconds ??? so long!

try one block file speed with one record.
val file2=sc.binaryRecords("hdfs://hadoop-master:9000/demo128/image_0_0_4096_4096_4_i16_envi",134217728)
file2.count()
3-4seconds !! nice!

try all files with one record. config 2 exec with 1 core
val all2=sc.binaryRecords("hdfs://hadoop-master:9000/demo128",134217728)
all2.count()
6.9GB(54*128mb) total use 53 seconds ! :) 

try all files with one record. config 2worker 2exec with 2 core
val all2=sc.binaryRecords("hdfs://hadoop-master:9000/demo128",134217728)
all2.count()
6.9GB(54*128mb) total use 90 seconds ! :) 
        worker wmem  exec  core  seconds
config   2     3 	2     2     bad
config   --    2	2     1     2.1 minus, mainly slowed by 178
config   --    1,2,2	2     1     master have too many task to work.

178 node is both worker and master, it slow down all process.
config   --    0,2,2     2     1   59s but master still have 4exec.
only2work --   2,2       2     1   72s

slaves 2, master 1 2exec 1core 53s
slaves 2, master 1 2exec 1core 47s
slaves 2, master 1 8exec 1core 53s this seems exec number setting not work for spark-shell without yarn.
slaves 2, no master 2exec 1core 72s

attension: i don't add hadoop-master in slaves, but it still in workerlist.
attension: it seems must reboot after modify spark-env.sh and slaves.

next two :
0. simple java program for spark.
http://spark.praveendeshmane.co.in/spark/spark-wordcount-java-example.jsp
1. write simple java program for spark. reading hdfs binary file.
2. use filename as key. https://stackoverflow.com/questions/29686573/spark-obtaining-file-name-in-rdds
3.read multi files https://www.tutorialkart.com/apache-spark/read-multiple-text-files-to-single-rdd/

jar run on cluster
 spark-submit --class SparkDemoOne --master spark://hadoop-master:7077 SparkDemoOne.jar
two workers only not include jar transfer time, use 75s.  
3 workers only not include jar transfer time, use __75s. 
3 workers only not include jar transfer time, use __70s. 
4 workers  only not include jar transfer time, use __s.

compute average of 6.7GB data:
c++ single thread use 75 seconds with about 100MB/s IO.
hadoop-yarn with 3 slave nodes use 95 seconds.
apache-spark with 3 workers use 75 seconds not including jar copying.
 
**************************************
**                                  **
**  HDFS Balance                    **
**                                  **
**************************************
add following properties to hdfs-site.xml

<property>
<name>dfs.balancer.max-size-to-move</name>
<value>134217728</value>
</property>
<property>
<name>dfs.datanode.fsdataset.volume.choosing.policy</name>
<value>org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy</value>
</property>
<property>
<name>dfs.ha.automatic-failover.enabled</name>
<value>false</value>
</property>
<property>
<name>dfs.disk.balancer.enabled</name>
<value>true</value>
</property>

use command after start-dfs.sh:
hdfs balancer -policy datanode -threshold 1.0


====Set Locality Level
set spark-env.sh each node with it's own name.
SPARK_LOCAL_HOSTNAME=<data node hostname>
SPARK_LOCAL_IP=<data node ip>
You can configure the wait time before moving to other locality levels using:
spark.locality.wait a big value

This is not working , many data run in diff node(22-32)seconds.

Try something for Process_local!! next.

add spark-env.sh
export SPARK_LOCALITY_WAIT=60s
export SPARK_LOCALITY_WAIT_PROCESS=1s

start worker manually
1.
/usr/local/spark/sbin/start-master.sh
2.
in each worker do:
ssh hadoop@hadoop-slave-1
/usr/local/spark/sbin/start-slave.sh -h hadoop-slave-1 spark://hadoop-master:7077

ssh hadoop@hadoop-slave-2
/usr/local/spark/sbin/start-slave.sh -h hadoop-slave-2 spark://hadoop-master:7077

ssh hadoop@hp
/usr/local/spark/sbin/start-slave.sh -h hp spark://hadoop-master:7077

===try1
hp use 4 cores, is very slow(10s one task, 11GB all use 2min ), I make it 2 cores and try.

ssh hadoop@hp
/usr/local/spark/sbin/start-slave.sh -h hp -c 2 spark://hadoop-master:7077

1.5min

===try2 each worker have 1 core.
/usr/local/spark/sbin/start-slave.sh -h hp -c 1 spark://hadoop-master:7077
/usr/local/spark/sbin/start-slave.sh -h hadoop-slave-1 -c 1 spark://hadoop-master:7077
/usr/local/spark/sbin/start-slave.sh -h hadoop-slave-2 -c 1 spark://hadoop-master:7077

total including jar copying, use 1.2min ~ 72s

===try3 add master as partial worker
/usr/local/spark/sbin/start-slave.sh -h hadoop-master -c 1 spark://hadoop-master:7077
total use 57s

===try3 4workers(1core) 215*128MB=26.875GB
total 2.2min=132seconds


*********************************
*                               *
* Try Seqfile Read performance  *
* sequence read                 *
* 2020-1-18                     *
*********************************
	ReadType	dura(sec)	spd(MB/s)
S	Key,Val		16.721	248.13
S	Only-Key	14.527	285.61
D	Key,Val		15.369	269.96
	Only-Key	14.484	286.45
	Key,Val		15.202	272.92
	Only-Key	14.567	284.82
	Key,Val		15.663	264.89
	Only-Key	14.741	281.46
			
D	Key,Val		76.547	54.20
I	Only-Key	71.993	57.63
S	Key,Val		77.488	53.54
K	Only-Key	73.136	56.73
	Key,Val		77.317	53.66
	Only-Key	73.527	56.43

Read total 4148.2MB seqfile. blockSize 128MB, each record 14.4MB.
Record count:288.
VMVare use SSD readspd 270MB/s, disk readspd 56MB/s.
Read Only-key use 1-2seconds less for SSD, 4-5seconds less for Disk.



****************************
*
* Try Seqfile Read performance
* random read with record interval x.
* 2020-1-18 
****************************
DISK	KV	49	42.33
inter2	K	44	47.14
	KV	46.5	44.60
	K	44.1	47.03
			
DISK	KV	25.3	40.99
inter4	K	21.8	47.57
	KV	24.3	42.68
	K	22.3	46.50
			
DISK	KV	1.04	498.58
inter8	K	0.76	682.27
	KV	1.36	381.27
	K	0.79	656.36
			
DISK	KV	0.66	392.82
inter16	K	0.65	398.87
	KV	0.71	365.16
	K	0.46	563.61
			
DISK	KV	0.16	180.04
inter	K	0.08	360.09
144	KV	0.2	144.03
	K	0.09	320.08


**************************************
**                                  **
**  应用自定义InputFormat            **
**  自定义RecordReader               **
**  自定义SeqFile存储Fy4 LST数据     **
**  自定义Stat对象存储Spark分析结果   **
**                                  **
**************************************
0.Fy4 4km LST(Int16)产品，一期全圆盘数据 2748*2748*2=14.40335MB 大约按14.5MB计。
一天按24小时，每5min一期，总计24h*12=288期，总计4.051GB 考虑包含syncmarker 大约按4.06GB计。
一天数据，普通SeqFile 33 个blocks。对齐后SeqfileB 36个blocks。
一个月普通121.5GB。

*** Section Y
Yarn 2020-1-21
Y-1.使用Yarn分析 一个月的数据121.5GB 8个节点：
yarn xxxx.jar param0 param1 param2...
Yarn默认启动了1个AM+31个Executor，一共32个容器。跑了540-580秒，215MB/s。
32个容器其中AM是随机选择的，并不一定在调用的节点或者Hadoop-Master节点。
默认Yarn通过每个容器的分配内存来计算全部可用容器数量，不考虑cpu的核数。
为了限制每个节点的容器数量，我给单个容器的内存增大到1500MB，这样每个节点
只有2个容器了。测试结果 700秒。每个节点2容器反而比4容器慢了，可能的解释为：
数据没有优先在本地处理，还有一个解释是启动和释放容器比较耗时间，2容器增加
了启动释放容器的时间。结论是Yarn怎么优化都不行，太慢了。

*** Section S
Spark 2020-1-22
使用spark提交一个Job
spark-submit --class com.company.Main --master spark://hadoop-master:7077 some.jar param0 param1 ...
使用spark分析121.5GB数据，每个节点采用2核和1核结果基本一致，结果如下（SEQ表示普通SeqFile）：
文件类型    总核数    耗时
SEQ        12        396s
SEQ        8         408-414s

问题及优化思路总结：
S-1.使用Seqfile文件（SEQ）保存一天的全部文件，使用Int作为键值（可以认为是观测时间），Bytes数组作为一期数据，
程序自动添加syncmarker（默认行为是100K加一个syncmarker，实际上除了第一条记录，后面每条记录都有syncmarker）。
每天SeqFile包含288条记录，共33个Block，存在跨Block的记录。每个记录的开始偏移值记录到文本文件中。

S-2.使用Spark进行Fy4数据（SEQ）统计分析时，spark处理的数据量增加到121.6GB（一个月30天数据）时，
大量数据影像存在跨block保存的情况，为了处理完整的影像，每个节点去其他节点抓取缺失的block。每个bock 128MB，各节点硬盘IO成为瓶颈。8节点统计一个月数据需要400秒。

S-3.Spark处理HDFS数据的性能分析。在HDFS中文件大小超过Block尺寸后会被分片，遥感数据以记录的形式存储在Block中。当数据记录处在两个Block的时候，Spark处理该任务的进程会读入两个Block，假设为Block1和Block2，共128MBx2=256MB的数据。有意思的是，由于进入新的Block，Spark在连处理下一条连续记录的时候，作为一个独立的子任务会新开一个进程，同时该进程并不会使用前面已经加载的Block2，而是从新读入Block2（此处应该有硬件层级的缓存），同时读入该Block2最后一条记录时遇到跨Block记录时，会继续读入Block3，那么本进程实际的IO仍然是256MB。可以看出Block2实际被读取了两次。
S-4.为了优化掉这个无效时间，我在把数据写入HDFS时严格保证每条记录不会出现在两个Block中，受限于HDFS底层API的限制，每个记录出现在跨Block的时候，不得不给上一条记录追加填充值，将这个记录完全挤到后一个Block中，我给这个技术命名为块对齐顺序文件存储（Block-Aligned Sequence File Storage），简写为BSEQ。虽然增加额外的无效的存储空间（一天数据量从4.05GB增加到4.5GB），但是经过测试，一天SeqFile处理时间可以缩短33%以上（见下表）。

使用T420i7虚拟机(机械硬盘测试)测试性能一天数据量：
文件类型		总核数		数据量		耗时
SEQ 		1		4.05GB 		120-126s         
BSEQ 		1		4.5GB 		90s

2020-1-23 在8节点测试BSEQ性能，分析一天与一个月数据。
文件类型		总核数		数据量		耗时
BSEQ 		9		4.5GB 		13-14s         
BSEQ 		9		135GB 		336-342s
BSEQ 		8		135GB		330-335s

模拟业务环境下一个月的风云四地表温度产品，大约135GB。对整月数据统计最大，最小和平均，最后八节点集群耗时330秒，比昨天没进行对齐的数据的处理时间缩短了70秒钟。平均每秒处理418MB，单节点平均52MB。虽然速度有提高，但是还是不够理想。增加核数并不能增加处理速度。

*** Section C
Custom Fy4 FileInputFormat RecordReader
C-1.针对SEQ,BSEQ进行MapReduce分析，由于默认的SeqFile的Mapreduce API
没法进行随机读取，只能顺序访问，因为用的是hadoop默认的FileInputFomat和RecordReader，
在Spark里也只能顺序访问，，那么即使分析一条记录，在map阶段也要遍历全部的记录。

C-2.使用普通BSEQ进行mapred均值分析，
一天数据4.5GB，block对齐，spark对结果take只取1个随机结果，
此时spark框架在map阶段访问了全部记录。耗时78-90秒。
因为要做reduce所以访问几条记录都会对全部记录进行map操作。

attension!!! 
C-3.使用seek定位seq文件中的记录时，如果这条记录包含sync marker，那么seek的地址就在sync marker位置上而不是记录的地址上。比如一个记录包含syncmarker，其sync地址是94，记录地址是114，
那么要seek到这条记录需要seek(94)。初步测试直接定位到对应记录需要10-11秒。
在Map阶段，分配到每个节点的Fy4RecordReader自己都没法带参数，每个节点的RecordReader都是初始状态，
为了不分析无效数据，必须让Fy4RecordReader获取记录偏移值。
但是如何动态修改recordreader的启示记录位置尚没有找到方法(已经找到方法见下文)。

attension!!! 使用jobConf携带master中的参数
C-4.可以通过sc.hadoopRDD中的jobconfig参数将自己的参数带到inputformat和recordreader里面，而且不受运行节点在哪里的限制。夹带私货十分的方便。


**************************************
**                                  **
**  Spark Streaming                 **
**                                  **
**************************************
Spark流处理
2020-1-27
使用流处理可以省略掉启动spark集群和各worker的时间，通过http拿到需要分析的键值，
直接到hdfs定位数据进行处理分析并返回结果是必然的路子。

2020-1-30 初步SparkStreaming研究城固偶：
attension!!!
1.如果在单机模拟流计算，至少需要给运行流计算的机器分配两个worker或者2 cores。
start-slave.sh -h ubuntu -c 2 spark://ubuntu:7077
只有一个core的话，SparkStreaming就使用这个core监听端口不做其他事情，
streaming接收到的任务一直在排队永远不会被处理。
2.运行流计算的命令：
首先要启动一个http流的服务进程，使用netcat，nc命令；然后提交spark任务。
如果没有对应端口的服务进程，java会返回port拒绝服务的异常。
在一个终端启动一个http服务：
>nc -l ubuntu 7777

在另一个终端提交spark任务
>spark-submit --class com.company.Main --master spark://ubuntu:7077 some.jar param0 param1....

此时在第一个终端可以输入字符串流，spark的终端会接收并处理。
重要：spark运行节点至少要有两个核可以使用，否则只接收7777的输入而不处理！！
如果只想用本地模式处理，可以在代码中给config项配置setMaster("Local[2]")。

下一步测试spark join转换与流计算配合使用，http只负责传递关键字，流计算中DStream转换函数负责定位hdfs数据资源进行处理，结果再返回给http流或者队列（MQ）服务。

2020-2-1 12:00 一点感想思路
SparkStreaming还有一个简单问题需要验证。在Driver程序了，初始化遥感数据RDD数据集，然后每个批次的Streaming RDD读入关键字和运算操作，将Streaming中的关键字与RDD关联，然后对RDD进行运算操作，结果以二进制流返回或者保存临时文件。需要检验，上面所说的RDD是否是分布式保存的？还是RDD只在一个节点保存。

一点测试工作
通过提交一个1024*1024*2 120条记录的测试数据验证了，使用streaming 关键字关联RDD的操作可行。
在单节点使用sparkStreaming验证原型程序的时候需要注意：
1.遇到下面异常时，表示程序运行过程中有Executor在运行过程中崩溃了，导致崩溃的原因很可能时分配内存不足。
org.apache.spark.shuffle.MetadataFetchFailedException: 
Missing an output location for shuffle 4
2.进行sparkStreaming测试时，至少需要3个Executor，一个负责监控流，一个负责Map，一个负责归并（reduce）操作。
假设如果单机分配了4个核，而spark-env.sh SPARK_EXECUTOR_MEMORY 配置了1024m的内存，那么该机器可用内存必须
大于4*1024m，否则spark运行中会出现executor无法得到内存而崩溃的错误。这里我的vm虚机为2核4GB，为了验证测试程序
我给该单节点slave启动了4个核，配置SPARK_EXECUTOR_MEMORY=512m ， 这样在sparkStreaming运行时保证至少有2GB
可用内存即可完成验证工作。




